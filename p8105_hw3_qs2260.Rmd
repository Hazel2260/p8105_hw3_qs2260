---
title: "p8105_hw3_qs2260"
author: "Qingzhen Sun"
date: "2022-10-11"
output: github_document
---

```{r}
library(tidyverse)
library(patchwork)
library(dplyr)
library(ggplot2)
library(ggridges)
library(p8105.datasets)

```
# problem 1
```{r}

data("instacart")

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```
```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```





# Problem 2

Sum up 24 hour activity
```{r}
tidy_acce = read_csv(file = "data/accel_data.csv")%>% 
  janitor::clean_names()%>%
  mutate(classify = ifelse((day == "Saturday"|day == "Sunday"),"weekend","weekday"))%>%
  mutate(total_act = rowSums(.[4:1443]))
```
made up table and draw plot
```{r}
tidy_df =tidy_acce %>% 
  select(week, day, classify, total_act)%>%
  arrange(total_act)
tidy_gf = tidy_acce %>%
  pivot_longer(activity_1:activity_1440,
               names_to = 'minutes',
               values_to = 'act') %>%
  ggplot(aes(x=as.integer(sub("activity_","",minutes)), y= act, color = day))+
  geom_line(alpha=0.5)

tidy_gf

```
After cleaning the dataset, the tidy_acce table lefts 35 rows and 1445 columns, which contains 1443 varibles includes week, day_id, day and 1440 minutes activity. 

The the table tidy_df shows the total activity of each day, which shows that the week and day is not really related with total activity.

Based on the ggplot result, it can be concluded that the most higher activity are mainly focus on 1200 minutes, and the lowest activity are located at around 200 minutes. 
# Problem 3

```{r}
tidy_noaa = read_csv(file = "data/nynoaadat.csv")%>% 
  janitor::clean_names()%>%
  separate(date, into = c("year", "month", "day"))%>%
  mutate(prcp = prcp / 10 ,
    tmax = as.numeric(tmax) /10,
    tmin = as.numeric(tmin) /10,
    year = as.numeric(year),
    day = as.numeric(day))
  
```

```{r}
names(which.max(table(tidy_noaa$snow)))
```
The most commonly observed value for snowfall is 0, since most of time there's not snow day which do not have snowfall.
```{r}
tidy_mon = tidy_noaa%>%
  filter(month == c("01", "07"))%>%
  group_by(id, year, month)%>%
  summarize(ave_tmax = mean(tmax, na.rm=TRUE))%>%
  ggplot(aes(x=year, y=ave_tmax, color=month))+
  geom_point()+
  geom_smooth(alpha=0.5)+
  facet_grid(.~month)
  

tidy_mon
```

Through the two-panel plot above, the temperature of july is higher than january obviously. Also, the stability of july is better than january, since the fluctuation of january id larger. On average, the max temperature of july is about 25 degrees higher than january.

```{r}
min_max=ggplot(tidy_noaa, aes(x=tmin, y=tmax))+geom_hex()

snowfall= tidy_noaa%>%
  filter(0< snow, snow<100)%>%
  mutate(year = as.factor(year))%>%
  ggplot(aes(x=snow, color=year))+
  geom_density()

snowfall/min_max
```

